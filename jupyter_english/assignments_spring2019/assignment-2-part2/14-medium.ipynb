{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wtf/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy import sparse\n",
    "import gensim\n",
    "from gensim.matutils  import Sparse2Corpus\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# conda install -c anaconda gensim \n",
    "# conda install -c conda-forge lightgbm \n",
    "#import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will help to throw away all HTML tags from article content/title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supplementary function to read a JSON line without crashing on escape characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_line(line=None):\n",
    "    result = None\n",
    "    try:        \n",
    "        result = json.loads(line)\n",
    "    except Exception as e:      \n",
    "        # Find the offending character index:\n",
    "        idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))      \n",
    "        # Remove the offending character:\n",
    "        new_line = list(line)\n",
    "        new_line[idx_to_replace] = ' '\n",
    "        new_line = ''.join(new_line)     \n",
    "        return read_json_line(line=new_line)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a JSON and forms a txt file leaving only article titles. When you resort to feature engineering and extract various features from articles, a good idea is to modify this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fields available in the json_data:\n",
    "\n",
    "```js\n",
    "['_id', '_timestamp', '_spider', 'url', 'domain', 'published',\n",
    " 'title', 'content', 'author', 'image_url', 'tags', 'link_tags', 'meta_tags']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tags_from_json(path_to_inp_json_file, path_to_out_txt_file, total_length):\n",
    "    '''\n",
    "    :param path_to_inp_json_file: path to a JSON file with train/test data\n",
    "    :param path_to_out_txt_file: path to extracted features (here titles), one per line\n",
    "    :param total_length: we'll pass the hardcoded file length to make tqdm even more convenient\n",
    "    '''\n",
    "    with open(path_to_inp_json_file, encoding='utf-8') as inp_file, \\\n",
    "         open(path_to_out_txt_file, 'w', encoding='utf-8') as out_file:\n",
    "        for line in tqdm_notebook(inp_file, total=total_length):\n",
    "            json_data = read_json_line(line)\n",
    "            content = json_data['content'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            tags_str = []\n",
    "            soup = BeautifulSoup(content, 'lxml')\n",
    "            try:\n",
    "                tag_block = soup.find('ul', class_='tags')\n",
    "                tags = tag_block.find_all('a')\n",
    "                for tag in tags:\n",
    "                    tags_str.append(tag.text.translate({ord(' '):None, ord('-'):None}))\n",
    "                tags = ' '.join(tags_str)\n",
    "            except Exception:\n",
    "                tags = 'None'\n",
    "                \n",
    "            out_file.write(' '.join(tags_str) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9eee0517ca4c2aa0320bcbbfa65cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=62313), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 5min 43s, sys: 2.56 s, total: 5min 46s\n",
      "Wall time: 5min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extract_tags_from_json(path_to_inp_json_file=os.path.join(PATH_TO_DATA, 'train.json'),\n",
    "           path_to_out_txt_file='train_tags.txt', total_length=62313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7831d400c0d460ca25423defe537350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=34645), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 3min 10s, sys: 1.37 s, total: 3min 11s\n",
      "Wall time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extract_tags_from_json(path_to_inp_json_file=os.path.join(PATH_TO_DATA, 'test.json'),\n",
    "           path_to_out_txt_file='test_tags.txt', total_length=34645)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain_from_json(path_to_inp_json_file, path_to_out_txt_file, total_length):\n",
    "    '''\n",
    "    :param path_to_inp_json_file: path to a JSON file with train/test data\n",
    "    :param path_to_out_txt_file: path to extracted features (here titles), one per line\n",
    "    :param total_length: we'll pass the hardcoded file length to make tqdm even more convenient\n",
    "    '''\n",
    "    with open(path_to_inp_json_file, encoding='utf-8') as inp_file, \\\n",
    "         open(path_to_out_txt_file, 'w', encoding='utf-8') as out_file:\n",
    "        for line in tqdm_notebook(inp_file, total=total_length):\n",
    "            json_data = read_json_line(line)\n",
    "            domain = json_data['domain'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            out_file.write(domain + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2527bd88224127b8c99fc1275a50bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=62313), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 12.2 s, sys: 562 ms, total: 12.8 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extract_domain_from_json(path_to_inp_json_file=os.path.join(PATH_TO_DATA, 'train.json'),\n",
    "           path_to_out_txt_file='train_domain.txt', total_length=62313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe9a8a6fd844c00b497ef6973f9b2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=34645), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 6.8 s, sys: 310 ms, total: 7.11 s\n",
      "Wall time: 7.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extract_domain_from_json(path_to_inp_json_file=os.path.join(PATH_TO_DATA, 'test.json'),\n",
    "           path_to_out_txt_file='test_domain.txt', total_length=34645)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_url_from_json(path_to_inp_json_file, path_to_out_txt_file, total_length):\n",
    "    '''\n",
    "    :param path_to_inp_json_file: path to a JSON file with train/test data\n",
    "    :param path_to_out_txt_file: path to extracted features (here titles), one per line\n",
    "    :param total_length: we'll pass the hardcoded file length to make tqdm even more convenient\n",
    "    '''\n",
    "    with open(path_to_inp_json_file, encoding='utf-8') as inp_file, \\\n",
    "         open(path_to_out_txt_file, 'w', encoding='utf-8') as out_file:\n",
    "        for line in tqdm_notebook(inp_file, total=total_length):\n",
    "            json_data = read_json_line(line)\n",
    "            #tags = json_data['tags'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            #print(json_data['image_url'])\n",
    "            if json_data['image_url'] != None:\n",
    "                image_url = '1'\n",
    "                #image_url = json_data['image_url'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            else:\n",
    "                image_url = '0'\n",
    "            out_file.write(image_url + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ab2ba6432340a8b2e6f93d28b018cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=62313), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 12.2 s, sys: 557 ms, total: 12.8 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extract_image_url_from_json(path_to_inp_json_file=os.path.join(PATH_TO_DATA, 'train.json'),\n",
    "           path_to_out_txt_file='train_image_url.txt', total_length=62313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4700d154f29440bd83c9da2e1ad8f335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=34645), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 6.81 s, sys: 309 ms, total: 7.12 s\n",
      "Wall time: 7.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extract_image_url_from_json(path_to_inp_json_file=os.path.join(PATH_TO_DATA, 'test.json'),\n",
    "           path_to_out_txt_file='test_image_url.txt', total_length=34645)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    \n",
    "    # Tokenize the string into words\n",
    "    #tokens = word_tokenize(text)\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Remove non-alphabetic tokens, such as punctuation\n",
    "    words = [word.lower() for word in tokens if word.isalpha()]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_from_json(path_to_inp_json_file, path_to_out_txt_file, total_length):\n",
    "    '''\n",
    "    :param path_to_inp_json_file: path to a JSON file with train/test data\n",
    "    :param path_to_out_txt_file: path to extracted features (here titles), one per line\n",
    "    :param total_length: we'll pass the hardcoded file length to make tqdm even more convenient\n",
    "    '''\n",
    "    with open(path_to_inp_json_file, encoding='utf-8') as inp_file, \\\n",
    "         open(path_to_out_txt_file, 'w', encoding='utf-8') as out_file:\n",
    "        for line in tqdm_notebook(inp_file, total=total_length):\n",
    "            json_data = read_json_line(line)\n",
    "            content = json_data['content'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            content_no_html_tags = remove_punctuation(strip_tags(content))\n",
    "            out_file.write(content_no_html_tags + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abee04bebdfd4ee2b2c82156181fa51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=62313), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 3min 22s, sys: 1.76 s, total: 3min 24s\n",
      "Wall time: 3min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extract_content_from_json(path_to_inp_json_file=os.path.join(PATH_TO_DATA, 'train.json'),\n",
    "           path_to_out_txt_file='train_content.txt', total_length=62313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3483b776e0764da79f3727e0483cf7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=34645), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 52s, sys: 975 ms, total: 1min 53s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extract_content_from_json(path_to_inp_json_file=os.path.join(PATH_TO_DATA, 'test.json'),\n",
    "           path_to_out_txt_file='test_content.txt', total_length=34645)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_titles_from_json(path_to_inp_json_file, path_to_out_txt_file, total_length):\n",
    "    '''\n",
    "    :param path_to_inp_json_file: path to a JSON file with train/test data\n",
    "    :param path_to_out_txt_file: path to extracted features (here titles), one per line\n",
    "    :param total_length: we'll pass the hardcoded file length to make tqdm even more convenient\n",
    "    '''\n",
    "    with open(path_to_inp_json_file, encoding='utf-8') as inp_file, \\\n",
    "         open(path_to_out_txt_file, 'w', encoding='utf-8') as out_file:\n",
    "        for line in tqdm_notebook(inp_file, total=total_length):\n",
    "            json_data = read_json_line(line)\n",
    "            content = json_data['title'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            #content_no_html_tags = remove_punctuation(strip_tags(content))\n",
    "            #content_no_html_tags = remove_punctuation(strip_tags(content))\n",
    "            #out_file.write(content_no_html_tags + '\\n')\n",
    "            out_file.write(content + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b625cc75ff47f2bd7921e6474d62ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=62313), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 12.4 s, sys: 571 ms, total: 12.9 s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extract_titles_from_json(path_to_inp_json_file=os.path.join(PATH_TO_DATA, 'train.json'),\n",
    "           path_to_out_txt_file='train_titles.txt', total_length=62313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dfeddd2242c4a74b559fc23836a5eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=34645), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 6.93 s, sys: 316 ms, total: 7.25 s\n",
      "Wall time: 7.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extract_titles_from_json(path_to_inp_json_file=os.path.join(PATH_TO_DATA, 'test.json'),\n",
    "           path_to_out_txt_file='test_titles.txt', total_length=34645)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head -5 ./train_titles.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_authors_from_json(path_to_inp_json_file, path_to_out_txt_file, total_length):\n",
    "    '''\n",
    "    :param path_to_inp_json_file: path to a JSON file with train/test data\n",
    "    :param path_to_out_txt_file: path to extracted features (here titles), one per line\n",
    "    :param total_length: we'll pass the hardcoded file length to make tqdm even more convenient\n",
    "    '''\n",
    "    with open(path_to_inp_json_file, encoding='utf-8') as inp_file, \\\n",
    "         open(path_to_out_txt_file, 'w', encoding='utf-8') as out_file:\n",
    "        for line in tqdm_notebook(inp_file, total=total_length):\n",
    "            json_data = read_json_line(line)\n",
    "            # {'name': None, 'url': 'https://medium.com/@Medium', 'twitter': '@Medium'}\n",
    "            authors = json_data['author']['url'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            out_file.write(authors + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd9f81ca30f4a6b823829658b559372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=62313), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 12.3 s, sys: 569 ms, total: 12.9 s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extract_authors_from_json(path_to_inp_json_file=os.path.join(PATH_TO_DATA, 'train.json'),\n",
    "           path_to_out_txt_file='train_authors.txt', total_length=62313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8787538470144448fb776d16b44f522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=34645), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 6.93 s, sys: 317 ms, total: 7.25 s\n",
      "Wall time: 7.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extract_authors_from_json(path_to_inp_json_file=os.path.join(PATH_TO_DATA, 'test.json'),\n",
    "           path_to_out_txt_file='test_authors.txt', total_length=34645)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_published_from_json(path_to_inp_json_file, path_to_out_txt_file, total_length):\n",
    "    '''\n",
    "    :param path_to_inp_json_file: path to a JSON file with train/test data\n",
    "    :param path_to_out_txt_file: path to extracted features (here titles), one per line\n",
    "    :param total_length: we'll pass the hardcoded file length to make tqdm even more convenient\n",
    "    '''\n",
    "    with open(path_to_inp_json_file, encoding='utf-8') as inp_file, \\\n",
    "         open(path_to_out_txt_file, 'w', encoding='utf-8') as out_file:\n",
    "        for line in tqdm_notebook(inp_file, total=total_length):\n",
    "            json_data = read_json_line(line)\n",
    "            # {'$date': '2015-08-03T07:44:50.331Z'}\n",
    "            published = json_data['published']['$date'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            out_file.write(published + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f8fb3eacd84358bd74d9d7473f0e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=62313), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 12.3 s, sys: 562 ms, total: 12.9 s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extract_published_from_json(path_to_inp_json_file=os.path.join(PATH_TO_DATA, 'train.json'),\n",
    "           path_to_out_txt_file='train_published.txt', total_length=62313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a0e8bde27d41668a4df84a1b9e7188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=34645), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 6.92 s, sys: 313 ms, total: 7.24 s\n",
      "Wall time: 7.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extract_published_from_json(path_to_inp_json_file=os.path.join(PATH_TO_DATA, 'test.json'),\n",
    "           path_to_out_txt_file='test_published.txt', total_length=34645)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Tags features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = TfidfVectorizer(max_features=1000, ngram_range=(1, 1))\n",
    "#cv = CountVectorizer(ngram_range=(1, 1), max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 313 ms, sys: 6.27 ms, total: 319 ms\n",
      "Wall time: 319 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('./train_tags.txt') as input_train_file:\n",
    "    X_train_tags_sparse = cv.fit_transform(input_train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 172 ms, sys: 3.25 ms, total: 175 ms\n",
      "Wall time: 175 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('./test_tags.txt') as input_train_file:\n",
    "    X_test_tags_sparse = cv.transform(input_train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Content features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words=\"english\"\n",
    "#tfidf = TfidfVectorizer(max_features=100000, ngram_range=(1, 2), analyzer=\"word\", stop_words=\"english\")\n",
    "tfidf = TfidfVectorizer(max_features=200000, ngram_range=(1, 3))\n",
    "# old with 1,2 3 min 7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 30s, sys: 3min 53s, total: 13min 23s\n",
      "Wall time: 15min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('./train_content.txt') as input_train_file:\n",
    "    X_train_content_sparse = tfidf.fit_transform(input_train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 24s, sys: 1.92 s, total: 1min 25s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#with open('./test_content.txt', encoding='utf-8') as input_train_file:\n",
    "with open('./test_content.txt') as input_train_file:\n",
    "    X_test_content_sparse = tfidf.transform(input_train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add title features\n",
    "We'll use a very simple feature extractor – TfidfVectorizer, meaning that we resort to the Bag-of-Words approach. For now, we are leaving only 50k features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf = TfidfVectorizer(max_features=100000, ngram_range=(1, 2))\n",
    "tfidf = TfidfVectorizer(max_features=200000, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.93 s, sys: 65.2 ms, total: 2.99 s\n",
      "Wall time: 1.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('train_titles.txt', encoding='utf-8') as input_train_file:\n",
    "    X_train_title_sparse = tfidf.fit_transform(input_train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.32 s, sys: 17.1 ms, total: 1.34 s\n",
      "Wall time: 572 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('test_titles.txt', encoding='utf-8') as input_test_file:\n",
    "    X_test_title_sparse = tfidf.transform(input_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 200000), (34645, 200000))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_title_sparse.shape, X_test_title_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read targets from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv(os.path.join(PATH_TO_DATA, \n",
    "                                        'train_log1p_recommends.csv'), \n",
    "                           index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_target['log_recommends'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target is still somewhat skewed, though it's allready log1p-transformed (#claps with log1p transformation). Yet, we'll apply log1p once more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train, bins=30, alpha=.5, color='red', \n",
    "         label='original', range=(0,10));\n",
    "plt.hist(np.log1p(y_train), bins=30, alpha=.5, color='green', \n",
    "         label='log1p', range=(0,10));\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 3), max_features=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.15 s, sys: 119 ms, total: 4.27 s\n",
      "Wall time: 3.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('train_titles.txt', encoding='utf-8') as input_train_file:\n",
    "    X_train_title_count_sparse = cv.fit_transform(input_train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 709 ms, sys: 5.81 ms, total: 715 ms\n",
      "Wall time: 714 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('test_titles.txt', encoding='utf-8') as input_test_file:\n",
    "    X_test_title_count_sparse = cv.transform(input_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add author feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://medium.com/@Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://medium.com/@Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://medium.com/@aelcenganda</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            author\n",
       "0       https://medium.com/@Medium\n",
       "1       https://medium.com/@Medium\n",
       "2  https://medium.com/@aelcenganda"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_author_df = pd.read_csv('./train_authors.txt', header=None, names=['author'])\n",
    "train_author_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://medium.com/@HITRECORD.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://medium.com/@mariabustillos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://medium.com/@HITRECORD.org</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               author\n",
       "0   https://medium.com/@HITRECORD.org\n",
       "1  https://medium.com/@mariabustillos\n",
       "2   https://medium.com/@HITRECORD.org"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_author_df = pd.read_csv('./test_authors.txt', header=None, names=['author'])\n",
    "test_author_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a united dataframe of author data for one hot encoding the author data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# United dataframe of the author data \n",
    "full_author_df = pd.concat([train_author_df, test_author_df])\n",
    "\n",
    "# Index to split the training and test data sets\n",
    "idx_split = train_author_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoder = OneHotEncoder(categories='auto')\n",
    "X_full_author_sparse = onehotencoder.fit_transform(full_author_df).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96958, 45374)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full_author_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the authors back into training and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_author_sparse = X_full_author_sparse[:idx_split,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_author_sparse = X_full_author_sparse[idx_split:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62313, 45374)\n",
      "(34645, 45374)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_author_sparse.shape)\n",
    "print(X_test_author_sparse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_onehotencoder = OneHotEncoder(categories='auto', handle_unknown=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_author_sparse = author_onehotencoder.fit_transform(train_author_df).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62313, 32182)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_author_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_author_sparse = author_onehotencoder.transform(test_author_df).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34645, 32182)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_author_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add date time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_published_df = pd.read_csv('./train_published.txt',\n",
    "                           header=None, names=['published'], parse_dates=['published'])\n",
    "test_published_df = pd.read_csv('./test_published.txt',\n",
    "                           header=None, names=['published'], parse_dates=['published'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df):    \n",
    "    hour = df['published'].apply(lambda ts: ts.hour)\n",
    "    \n",
    "    morning = ((hour >= 7) & (hour <= 11)).astype('int')\n",
    "    day = ((hour >= 12) & (hour <= 18)).astype('int')\n",
    "    evening = ((hour >= 19) & (hour <= 23)).astype('int')\n",
    "    night = ((hour >= 0) & (hour <= 6)).astype('int')\n",
    "    \n",
    "    dayofweek = df['published'].apply(lambda ts: ts.dayofweek)\n",
    "    is_weekday = ((dayofweek == 5) | (dayofweek == 6)).astype('int')\n",
    "\n",
    "    onehotencoder = OneHotEncoder(categories='auto')\n",
    "    dayofweek_hot_encoded = onehotencoder.fit_transform(dayofweek.values.reshape(-1, 1)).toarray()\n",
    "    \n",
    "    #onehotencoder = OneHotEncoder(categories='auto')\n",
    "    #hour_published_hot_encoded = onehotencoder.fit_transform(hour.values.reshape(-1, 1)).toarray()\n",
    "\n",
    "    #month_published = df['published'].apply(lambda ts: ts.month).astype('int')\n",
    "    #onehotencoder = OneHotEncoder(categories='auto')\n",
    "    #month_published_hot_encoded = onehotencoder.fit_transform(month_published.values.reshape(-1, 1)).toarray()\n",
    "\n",
    "    \n",
    "    empty_sparse_matrix = csr_matrix((df.shape[0], 0))\n",
    "    X = hstack([empty_sparse_matrix,\n",
    "                morning.values.reshape(-1, 1),\n",
    "                day.values.reshape(-1, 1),\n",
    "                evening.values.reshape(-1, 1),\n",
    "                night.values.reshape(-1, 1),\n",
    "                #dayofweek_hot_encoded,\n",
    "                is_weekday.values.reshape(-1,1),\n",
    "                #month_published_hot_encoded # #ignore for now - the test set does not have data for all 12 months\n",
    "                #hour_published_hot_encoded  \n",
    "                ]).tocsr()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 471 ms, sys: 86.5 ms, total: 558 ms\n",
      "Wall time: 559 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_time_features_sparse = add_time_features(train_published_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 180 ms, sys: 1.96 ms, total: 182 ms\n",
      "Wall time: 181 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test_time_features_sparse = add_time_features(test_published_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add image_url feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_url\n",
       "0          0\n",
       "1          0\n",
       "2          1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_url_df = pd.read_csv('./train_image_url.txt', header=None, names=['image_url'])\n",
    "train_image_url_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_url\n",
       "0          1\n",
       "1          1\n",
       "2          1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image_url_df = pd.read_csv('./test_image_url.txt', header=None, names=['image_url'])\n",
    "test_image_url_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_image_url_sparse = csr_matrix(train_image_url_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_image_url_sparse = csr_matrix(test_image_url_df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add domain feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_domain_df = pd.read_csv('./train_domain.txt', header=None, names=['train_domain'])\n",
    "#train_domain_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_domain_df = pd.read_csv('./test_domain.txt', header=None, names=['train_domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_site(x, l):\n",
    "    if x in l:\n",
    "      return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def add_domain_feature(df):    \n",
    "    foo = pd.DataFrame(index=df.index)\n",
    "    foo['is_medium'] = df['train_domain'].apply(lambda x: is_site('medium.com', x))\n",
    "    foo['is_hackernoon'] = df['train_domain'].apply(lambda x: is_site('hackernoon.com', x))\n",
    "    return csr_matrix(foo.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_domain_sparse = add_domain_feature(train_domain_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62313, 2)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_domain_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_domain_sparse = add_domain_feature(test_domain_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34645, 2)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_domain_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join all sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.5 s, sys: 12.4 s, total: 40.9 s\n",
      "Wall time: 36.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = hstack([X_train_author_sparse,\n",
    "                  X_train_time_features_sparse,\n",
    "                  X_train_title_sparse,\n",
    "                  X_train_content_sparse,\n",
    "                  X_train_image_url_sparse,\n",
    "                  X_train_domain_sparse,\n",
    "                  X_train_tags_sparse,\n",
    "                  #X_train_title_lda,\n",
    "                  #X_train_title_count_sparse\n",
    "                ]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.2 s, sys: 8.37 s, total: 24.6 s\n",
      "Wall time: 20.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test = hstack([X_test_author_sparse,\n",
    "                 X_test_time_features_sparse,\n",
    "                 X_test_title_sparse,\n",
    "                 X_test_content_sparse,\n",
    "                 X_test_image_url_sparse,\n",
    "                 X_test_domain_sparse,\n",
    "                 X_test_tags_sparse,\n",
    "                 #X_test_title_lda\n",
    "                 #X_test_title_count_sparse\n",
    "                ]).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and model training\n",
    "Let's make a 30%-holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_part_size = int(0.7 * train_target.shape[0])\n",
    "X_train_part = X_train[:train_part_size, :]\n",
    "y_train_part = y_train[:train_part_size]\n",
    "X_valid =  X_train[train_part_size:, :]\n",
    "y_valid = y_train[train_part_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to fit a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(random_state=17, alpha=1.25, fit_intercept=False, solver=\"saga\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.6 s, sys: 530 ms, total: 23.2 s\n",
      "Wall time: 21.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.25, copy_X=True, fit_intercept=False, max_iter=None,\n",
       "   normalize=False, random_state=17, solver='saga', tol=0.001)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ridge.fit(X_train_part, np.log1p(y_train_part));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After log1p-transformation, we need to apply an inverse  expm1-transformation to predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pred = np.expm1(ridge.predict(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.052790496860926"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_valid_mae = mean_absolute_error(y_valid, ridge_pred)\n",
    "ridge_valid_mae # 1.052790496860926"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0542281512959446  # increased content max_features=200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_sparse_title_count_data =  sparse.vstack([X_train_title_count_sparse, X_test_title_count_sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform our sparse_data to corpus for gensim\n",
    "#corpus_data_gensim = gensim.matutils.Sparse2Corpus(full_sparse_title_count_data, documents_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#lda = LdaModel(corpus_data_gensim, num_topics = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#def document_to_lda_features(lda_model, document):\n",
    "#    topic_importances = lda.get_document_topics(document, minimum_probability=0)\n",
    "#    topic_importances = np.array(topic_importances)\n",
    "#    return topic_importances[:,1]\n",
    "\n",
    "#lda_features = list(map(lambda doc:document_to_lda_features(lda, doc),corpus_data_gensim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_pd_lda_features = pd.DataFrame(lda_features)\n",
    "#data_pd_lda_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_title_lda = csr_matrix(data_pd_lda_features.iloc[:y_train.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_title_lda = csr_matrix(data_pd_lda_features.iloc[y_train.shape[0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_title_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_title_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_content_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_sparse_content_count_data =  sparse.vstack([X_train_title_count_sparse, X_test_title_count_sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn = KNeighborsClassifier(n_neighbors=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv = CountVectorizer(ngram_range=(1, 3), max_features=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#with open('train_titles.txt', encoding='utf-8') as input_train_file:\n",
    "#    X_train_title_count_sparse = cv.fit_transform(input_train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#with open('./train_content.txt') as input_train_file:\n",
    "#    X_train_content_sparse = tfidf.fit_transform(input_train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baz = X_train_title_count_sparse.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#knn.kneighbors(X_train_title_count_sparse)\n",
    "#knn.fit(baz, y_train)\n",
    "#knn.fit(X_train_title_count_sparse, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we fit a LightGBM model with mean_absolute_error as objective (it's important!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgb_x_train_part = lgb.Dataset(X_train_part.astype(np.float32), label=np.log1p(y_train_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgb_x_valid = lgb.Dataset(X_valid.astype(np.float32), label=np.log1p(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param = {'num_leaves': 255, \n",
    "         'objective': 'mean_absolute_error',\n",
    "         'metric': 'mae'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's l1: 0.378947\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\tvalid_0's l1: 0.371241\n",
      "[3]\tvalid_0's l1: 0.364806\n",
      "[4]\tvalid_0's l1: 0.359429\n",
      "[5]\tvalid_0's l1: 0.354428\n",
      "[6]\tvalid_0's l1: 0.350511\n",
      "[7]\tvalid_0's l1: 0.346908\n",
      "[8]\tvalid_0's l1: 0.343523\n",
      "[9]\tvalid_0's l1: 0.340777\n",
      "[10]\tvalid_0's l1: 0.338537\n",
      "[11]\tvalid_0's l1: 0.335855\n",
      "[12]\tvalid_0's l1: 0.334217\n",
      "[13]\tvalid_0's l1: 0.332712\n",
      "[14]\tvalid_0's l1: 0.331353\n",
      "[15]\tvalid_0's l1: 0.329988\n",
      "[16]\tvalid_0's l1: 0.328736\n",
      "[17]\tvalid_0's l1: 0.327357\n",
      "[18]\tvalid_0's l1: 0.326148\n",
      "[19]\tvalid_0's l1: 0.325379\n",
      "[20]\tvalid_0's l1: 0.32433\n",
      "[21]\tvalid_0's l1: 0.323459\n",
      "[22]\tvalid_0's l1: 0.322929\n",
      "[23]\tvalid_0's l1: 0.322049\n",
      "[24]\tvalid_0's l1: 0.321273\n",
      "[25]\tvalid_0's l1: 0.320787\n",
      "[26]\tvalid_0's l1: 0.32033\n",
      "[27]\tvalid_0's l1: 0.319767\n",
      "[28]\tvalid_0's l1: 0.319204\n",
      "[29]\tvalid_0's l1: 0.318715\n",
      "[30]\tvalid_0's l1: 0.318412\n",
      "[31]\tvalid_0's l1: 0.317986\n",
      "[32]\tvalid_0's l1: 0.317689\n",
      "[33]\tvalid_0's l1: 0.317275\n",
      "[34]\tvalid_0's l1: 0.316883\n",
      "[35]\tvalid_0's l1: 0.316661\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-2c87d6a7ad07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbst_lgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlgb_x_train_part\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlgb_x_valid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    214\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1758\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1760\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1761\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#num_round = 200\n",
    "#bst_lgb = lgb.train(param, lgb_x_train_part, num_round, valid_sets=[lgb_x_valid], early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_pred = np.expm1(bst_lgb.predict(X_valid.astype(np.float32), num_iteration=bst_lgb.best_iteration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot predictions and targets for the holdout set. Recall that these are #recommendations (= #claps) of Medium articles with the  np.log1p transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_valid, bins=30, alpha=.5, color='red', label='true', range=(0,10));\n",
    "plt.hist(ridge_pred, bins=30, alpha=.5, color='green', label='Ridge', range=(0,10));\n",
    "plt.hist(lgb_pred, bins=30, alpha=.5, color='blue', label='Lgbm', range=(0,10));\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the prediction is far from perfect, and we get MAE  ≈  1.3 that corresponds to  ≈  2.7 error in #recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_valid_mae = mean_absolute_error(y_valid, ridge_pred)\n",
    "ridge_valid_mae # 1.0542281512959446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_valid_mae = mean_absolute_error(y_valid, lgb_pred)\n",
    "lgb_valid_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple blending\n",
    "Now let's mix predictions. We's just pick up weights 0.6 for Lgbm and 0.4 for Ridge, but these are typically tuned via cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_absolute_error(y_valid, .4 * lgb_pred + .6 * ridge_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train both models on the full accessible training set, make predictions for the test set and form submission files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ridge.fit(X_train, np.log1p(y_train));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ridge_test_pred = np.expm1(ridge.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgb_x_train = lgb.Dataset(X_train.astype(np.float32), label=np.log1p(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_round = 60\n",
    "#bst_lgb = lgb.train(param, lgb_x_train, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgb_test_pred = np.expm1(bst_lgb.predict(X_test.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mix_pred = .4 * lgb_test_pred + .6 * ridge_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaderboard probing\n",
    "Now we apply a dirty hack. Originally, we made you come up with it by your own (as a part of an assignment, with only a hint from out side), but now it's described in this tutorial, written within a previous session of mlcourse.ai.\n",
    "\n",
    "Submitting all zeros gives 4.33328. If you take a pen and a piece of paper and figure out what it means for MAE that all predictions are zeros, then you'll see that it's exactly the mean target value for the test set. We can compare it with mean target for training data and correspondingly adjust predictions. Looks like a dirty hack, however, the same thing is often done with time series prediction (even in production) - merely adjusting your predictions to a change in target variable distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test_target = 4.33328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mix_test_pred_modif = mix_pred + mean_test_target - y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_submission_file(prediction, filename,\n",
    "                          path_to_sample=os.path.join(PATH_TO_DATA, 'sample_submission.csv')):\n",
    "    submission = pd.read_csv(path_to_sample, index_col='id')\n",
    "    \n",
    "    submission['log_recommends'] = prediction\n",
    "    submission.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.43272\n",
    "write_submission_file(ridge_test_pred + mean_test_target - y_train.mean(), 'submissions/14-ridge_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_submission_file(mix_test_pred_modif, 'submissions/14-medium-submission-blending.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_submission_file(ridge_test_pred, 'submissions/03-medium-submission.csv') # 2.03034 # no probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_submission_file(lgb_test_pred + mean_test_target - y_train.mean(), 'submissions/03-lgb_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_submission_file(mix_test_pred_modif, 'submissions/03-medium-submission-probing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, simple blending decreases MAE for both holdout predictions and on the leaderboard. However, I don't recommend to play with blending/stacking schemes in the beginning of the competition – it's crucially important to come up with good features first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can further improve your model in various ways. I've described them in this kernel. Go and compete, good luck! https://www.kaggle.com/kashnitsky/ridge-countvectorizer-baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO implement this part from https://www.kaggle.com/kashnitsky/ridge-and-lightgbm-blending-hacking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
